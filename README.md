# ml-optimizers-pure-python
Implementing different type of Gradient descents from scratch 
# Optimizers from Scratch: Batch GD, Stochastic GD, and Mini-batch GD

This project is a hands-on implementation of core **optimization algorithms** used in machine learning — built entirely from scratch in **pure Python**. Inspired by concepts from *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* by **Aurélien Géron**, this repository demonstrates:

-  **Batch Gradient Descent**
-  **Stochastic Gradient Descent (SGD)**
-  **Mini-batch Gradient Descent**

All implementations are written from scratch **without using ML libraries** like `scikit-learn` or `TensorFlow` for training logic.

---

## 📚 What You’ll Learn

- How gradient descent algorithms work **under the hood**
- The **convergence behavior** of different optimizers
- The **trade-offs** between:
  - 🚅 Speed (SGD)
  - 🎯 Stability (Batch GD)
  - ⚖️ Balance (Mini-batch GD)
- How optimizer choice impacts model **training dynamics**

---

## ✨ Features

- ✅ Fully manual weight updates
- 💡 Deep insight into **training behavior and optimizer impact**

---

---


