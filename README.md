# ml-optimizers-pure-python
Implementing different type of Gradient descents from scratch 
# Optimizers from Scratch: Batch GD, Stochastic GD, and Mini-batch GD

This project is a hands-on implementation of core **optimization algorithms** used in machine learning â€” built entirely from scratch in **pure Python**. Inspired by concepts from *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* by **AurÃ©lien GÃ©ron**, this repository demonstrates:

-  **Batch Gradient Descent**
-  **Stochastic Gradient Descent (SGD)**
-  **Mini-batch Gradient Descent**

All implementations are written from scratch **without using ML libraries** like `scikit-learn` or `TensorFlow` for training logic.

---

## ğŸ“š What Youâ€™ll Learn

- How gradient descent algorithms work **under the hood**
- The **convergence behavior** of different optimizers
- The **trade-offs** between:
  - ğŸš… Speed (SGD)
  - ğŸ¯ Stability (Batch GD)
  - âš–ï¸ Balance (Mini-batch GD)
- How optimizer choice impacts model **training dynamics**

---

## âœ¨ Features

- âœ… Fully manual weight updates
- ğŸ’¡ Deep insight into **training behavior and optimizer impact**

---

---


